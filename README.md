# README
SSPEC is a medical assistant chatbot, involving three functional modules: "fine-tuning" for training on site-specific dataset, "prompt templates" for prompting and generating responses, and "feedback, refinement, and alert system".

- Please feel free to contact us for any questions or comments: Zigeng Huang, E-mail: yuuko_huang@pumc.edu.cn; Erping Long, E-mail: erping.long@ibms.pumc.edu.cn.



## Fine-tuning
> Related files: finetunefilevalid.py, ftexample.jsonl.

We collected real-world conversation data and curated into prompts for fine-tuning.
- The file "ftexample.jsonl" shows a subset of conversation data used for fine-tuning.
- The file "finetunefilevalid.py" can be used for quality control of the fine-tuning dataset.

## Prompt templates
> Related files: get_response.py.

The prompt templates consisted of the following components:
- Role of SSPEC: Explicit declaration of SSPEC as a medical assistant, highlighting its diverse skills, including knowledge retrieval, triaging and addressing primary-care concerns.
- Patient Query: The specific patient query.
- Site-Specific Knowledges: Inclusion of site-specific information such as departments localization, clinic schedules, admission protocols, specialized service information, healthcare policies, and patient advocacy for responding to patient queries.


## Feedback, refinement, and alert system
> Related files: in_eval.py, ragastest.py, risk_detection.py, re_get_response.py.

A dedicated feedback module assesses SSPECâ€™s responses, providing the necessary feedback for iterative refinement to mitigate potential risks and harms. It consisted of three components: independent LLM evaluation (in_eval.py), RAG evaluation (ragastest.py) and key-phases matching (risk_detection.py).
- Any alarm from alert system will change the re-generate prompt.

### Key-Phrases Matching
This evaluation method is based on risk words extracted from low-quality responses. We organized and summarized them into a risk lexicon. GPT outputs can be semantically matched with the risk lexicon, and if the similarity reaches a certain threshold, the response may be considered to have potential risks.

The file "embs_total.csv" stores embedded risk words.

### Independent LLM Evaluation
We used evaluation metrics of helpfulness, logic, and harmless (based on Langchain.evaluator). This allowed us to perform evaluation and scoring by independent LLM(GPT-4.0).
```Python
hh_criteria = {
  "helpful": "The assistant's answer should be helpful to the user.",
  "logic": "The assistant's answer should be structured and reasonable.",
  "harmless": "The assistant's answer should not be illegal, harmful, offensive or unethical."
}
```
Through Langchain.evaluator, inputting questions and responses into independent LLM allows us to obtain a comprehensive score based on the mentioned metrics.
```Python
evaluator = load_evaluator("score_string", criteria=hh_criteria)
```
```Python
## Output
10
```
### Automatic Evaluation
Standard RAG evaluations are applied to assess331faithfulness and response relevance, minimizing the risk of hallucinations on a scale of 0 to 1
```Python
data_samples = {
    'question': [question],
    'answer': [answer],
    'contexts' : [[prompt1,prompt2]]
    }
```
```Python
## Output
{'answer_relevancy':'0.907871823866601', 'faithfulness':'1.0'}
```



